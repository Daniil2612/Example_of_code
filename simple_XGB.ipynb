{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84895,"databundleVersionId":10008389,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ПРОГНОЗИРОВАНИЕ ВОЗМОЖНОСТИ ДЕПРЕССИИ\n\nВ этой тетради мы изучаем данные о психическом здоровье из опроса, чтобы проанализировать и разработать модель, которая поможет предсказать вероятность депрессии.\n\nЭто тетрадь для начинающих, чтобы понять и познакомиться с МО и прогнозным анализом, и мы сосредоточимся на построении модели XGBoost для обучения и подгонки наших данных. В общем, получайте удовольствие и проявляйте творческий подход!\n\n### Сначала мы импортируем все необходимые библиотеки.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-18T12:21:02.312678Z","iopub.execute_input":"2024-11-18T12:21:02.313526Z","iopub.status.idle":"2024-11-18T12:21:02.319225Z","shell.execute_reply.started":"2024-11-18T12:21:02.313485Z","shell.execute_reply":"2024-11-18T12:21:02.318112Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Сохранение и изменение данных\n\nТеперь мы читаем исходные данные, устанавливаем нашу цель и выбираем наши предикторы. Мы также разделяем категориальные и числовые столбцы и выбираем столбцы с низкой кардинальностью для использования в эффективном прогнозировании.","metadata":{}},{"cell_type":"code","source":"# Loading all data \n\nX_full = pd.read_csv(\"/kaggle/input/playground-series-s4e11/train.csv\", index_col='id')\nX_test_full = pd.read_csv(\"/kaggle/input/playground-series-s4e11/test.csv\", index_col= 'id')\n\n# Separate target and predictors\n\nX_full.dropna(axis=0, subset=['Depression'], inplace=True)\ny = X_full.Depression\nX_full.drop(['Depression'], axis=1, inplace=True)\n\n# Split test data\n\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, train_size=0.8, \n                                                                test_size=0.2, \n                                                                random_state=0)\n\n# Select only low cardinality categorical columns and numerical columns for analysis and prediction\n\ncategorical_cols = [cname for cname in X_train_full.columns if\n                    X_train_full[cname].nunique() < 10 and \n                    X_train_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if \n                X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T12:21:02.321708Z","iopub.execute_input":"2024-11-18T12:21:02.322676Z","iopub.status.idle":"2024-11-18T12:21:03.142261Z","shell.execute_reply.started":"2024-11-18T12:21:02.322627Z","shell.execute_reply":"2024-11-18T12:21:03.140989Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let's take a peek at our training data.","metadata":{}},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T12:21:03.143745Z","iopub.execute_input":"2024-11-18T12:21:03.144219Z","iopub.status.idle":"2024-11-18T12:21:03.16471Z","shell.execute_reply.started":"2024-11-18T12:21:03.144168Z","shell.execute_reply":"2024-11-18T12:21:03.163509Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Предварительная обработка\n\nПроще всего использовать конвейеры для поддержания чистого и проверяемого кода. Выполняет необходимые подстановки, кодирование.","metadata":{}},{"cell_type":"code","source":"# Numerical transformer\nnum_trans = SimpleImputer(strategy = 'constant')\n\n# Categorical Transformer\ncat_trans = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('ohen', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_trans, numerical_cols),\n        ('cat', cat_trans, categorical_cols)\n    ])\n\n# One-hot encode the data\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_test = pd.get_dummies(X_test)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T12:21:03.16635Z","iopub.execute_input":"2024-11-18T12:21:03.166788Z","iopub.status.idle":"2024-11-18T12:21:03.295069Z","shell.execute_reply.started":"2024-11-18T12:21:03.166749Z","shell.execute_reply":"2024-11-18T12:21:03.294038Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Модель обучения и подбора\n\nМы разрабатываем модель путем тонкой настройки параметров и обучаем модель с использованием наших обучающих данных. Затем мы проверяем ее с использованием наших данных проверки для прогнозирования и вычисляем среднюю абсолютную ошибку с набором проверки y.","metadata":{}},{"cell_type":"code","source":"# Define model\n\nmodel  = XGBRegressor(n_estimators = 900, early_stopping_rounds = 4, learning_rate = 0.03)\n\n# Fit the model \n\nmodel.fit(X_train, y_train, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)\n\n# Get predictions\npredictions = model.predict(X_valid)\n\n# Calculate MAE\nmae = mean_absolute_error(predictions, y_valid)\n\nprint(\"Mean Absolute Error:\" , mae)","metadata":{"execution":{"iopub.status.busy":"2024-11-18T12:21:03.297648Z","iopub.execute_input":"2024-11-18T12:21:03.298115Z","iopub.status.idle":"2024-11-18T12:21:25.349516Z","shell.execute_reply.started":"2024-11-18T12:21:03.298067Z","shell.execute_reply":"2024-11-18T12:21:25.348341Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save test predictions to file\ntest_preds = model.predict(X_test)\noutput = pd.DataFrame({'id': X_test.index,\n                       'Depression': test_preds.round()})\noutput.to_csv('submission.csv', index=False)\nprint(\"And we're done! Good luck to everybody !!\")","metadata":{"execution":{"iopub.status.busy":"2024-11-18T12:23:27.210559Z","iopub.execute_input":"2024-11-18T12:23:27.211022Z","iopub.status.idle":"2024-11-18T12:23:27.795106Z","shell.execute_reply.started":"2024-11-18T12:23:27.21098Z","shell.execute_reply":"2024-11-18T12:23:27.793895Z"},"trusted":true},"outputs":[],"execution_count":null}]}